{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating reproducible data science workflows with DVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tutorial [link](https://medium.com/y-data-stories/creating-reproducible-data-science-workflows-with-dvc-3bf058e9797b)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Despite being implemented in software, development of data science and machine learning projects is dramatically different from general-purpose software development: for example, DS is primarily experiment-driven and has a much higher level of intrinsic unpredictability.\n",
    "\n",
    "Data science and machine learning are very different. Instead of features to implement, in ML we have ideas to try out, without any guarantee of succeeding. Before long, you may test a few dozen hypotheses, often substantially varying in their complexity and the implementation required. Most of them will fail, some may look promising, and ultimately, some complicated combinations of them may succeed.\n",
    "\n",
    "Eventually, you or your colleague will try to reproduce one of those models, the one which seems to be the best. You may find out that you cannot easily reconstruct how exactly that model was created. It may turn out that the training parameters are buried somewhere in a notebook, overwritten by subsequent experiments and multiple Git commits in several branches. In addition, perhaps the training/cross-validation split was performed without setting a random seed. And if you’re still not worried enough, the cherry on top is that when examined, it turns out the current features look nothing like what they were when the model was created. Too bad."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Science processes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some teams use the variants of [Coockiecutter Data Science](https://drivendata.github.io/cookiecutter-data-science/), others follow the approach outlined in [Guerilla Analytics](http://guerrilla-analytics.net/the-principles/). Both address the same problems, both offer powerful tools and we recommend you to try them out.  \n",
    "\n",
    "Chances are, you or your team already use something similar. But regardless of which approach you use to write reproducible data science code, you need tooling. \n",
    "\n",
    "The bare minimum requirements are the following:\n",
    "- a way to **version-control the data**, especially intermediate artefacts like pre-computed features and models,\n",
    "- being able to **pass data files inside the team** in a controllable and trackable way,\n",
    "- a tool to **reproduce any artefacts or results**, in a simple and automated way, regardless of how long ago they were originally created."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** Other terms like documents, deliverables, or work products are widely used in software development communities instead of the term **artefact**. \n",
    "\n",
    "More: https://www.quora.com/What-does-the-term-artefact-mean-in-software-engineering-or-programming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial we will explore, how DVC implements all of the processes we’ve outlined and makes reproducible data science easier. DVC is open-source and attempts to be a Git for machine learning, while working closely together with Git itself.  \n",
    "\n",
    "**DVC** is actually quite a large tool, but the most common operations are simple enough, in the same way that basic Git commands are easy to learn and incorporate into daily practice.  \n",
    "\n",
    "DVC is not the only tool for the job. It works best for small to middle-sized projects and solves the problem without adding too much complexity. However, depending on your needs, project size and deployment considerations you may find Kedro or other tools more suitable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Versioning the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s first define what **data version control** is:\n",
    "- state of any data file, whether original one or derived, must be recorded,\n",
    "- there must be a tool to switch between different versions of data files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the following scenario: the training data comes from a relational database and is stored as a CSV file. Once in a while, you want to update the dataset with recent records from the database. Each time you do so, you record the state of the dataset. If you have a way to *switch to any of the previous versions and back* — congratulations, your data is version controlled.  \n",
    "\n",
    "Git is not suitable for this, as it was not designed to serve large or binary files, while extensions like Git LFS are general-purpose and can be used for data version control only with some limitations and inconvenience. DVC offers a more flexible approach.  \n",
    "\n",
    "To illustrate it we will use [Titanic dataset](https://www.kaggle.com/c/titanic) from Kaggle and build a simple model and a submission file. With this miniature data science project, we will see, how **DVC helps to ensure data lineage and reproducibility**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install DVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install dvc\n",
    "# Or\n",
    "!conda install -c conda-forge dvc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> *Note, that you can configure DVC to use external storage to hold and exchange data, and in that case you’ll also need to install additional dependencies. For example, if you plan to use Amazon Web Services S3, you need to install boto and some other packages with*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install dvc[s3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Install [AWS CLI v2](https://docs.aws.amazon.com/cli/latest/userguide/install-cliv2.html)\n",
    "- Configure [AWS](https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-configure.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project Layout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our project starts with the Titanic dataset, which contains two files (one for training and one for testing), and the project structure.  \n",
    "\n",
    "It may be tempting to have all the data files and code in the same directory for the project this small.  \n",
    "\n",
    "However, it’s strategically wiser to stick to the same project structure for any project, regardless of its size. A disciplined approach to project structure and operations saves a lot of headache and time along the road.\n",
    "\n",
    "Let’s create a skeleton for our project:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir titanic-dvc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd titanic-dvc/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: ./titanic-dvc/data: File exists\r\n",
      "mkdir: ./titanic-dvc/features: File exists\r\n",
      "mkdir: ./titanic-dvc/results: File exists\r\n",
      "mkdir: ./titanic-dvc/pytitanic: File exists\r\n"
     ]
    }
   ],
   "source": [
    "!mkdir './titanic-dvc/'{data,features,results,pytitanic}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "!touch './titanic-dvc/README.md'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34m./titanic-dvc/\u001b[00m\r\n",
      "├── README.md\r\n",
      "├── \u001b[01;34mdata\u001b[00m\r\n",
      "├── \u001b[01;34mfeatures\u001b[00m\r\n",
      "├── \u001b[01;34mpytitanic\u001b[00m\r\n",
      "└── \u001b[01;34mresults\u001b[00m\r\n",
      "\r\n",
      "4 directories, 1 file\r\n"
     ]
    }
   ],
   "source": [
    "!tree './titanic-dvc/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The original data goes in the `data` directory. Although we have only two original data files, for larger projects there may be tens, thousands or even millions of data files, so it’s reasonable to have a separate directory for them.\n",
    "\n",
    "All derived features and intermediate data files go to `features` directory. Results (for example, trained models and submission files) will live in `results` directory.\n",
    "\n",
    "`pytitanic` directory will contain Python code for the project: scripts, modules, packages, etc. Additionally, you may have `notebooks` directory, and directories for code in other languages (for example, in R or Julia).\n",
    "\n",
    "We will keep `README.md` empty out of simplicity, although we still create it for the sake of the general procedure.\n",
    "\n",
    "To finish the project setup, we need to add data files and initialize Git repository and DVC. Given that you have already downloaded the data as a Zip archive in data directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  ./titanic-dvc/data/titanic.zip\r\n",
      "  inflating: ./titanic-dvc/data/gender_submission.csv  \r\n",
      "  inflating: ./titanic-dvc/data/test.csv  \r\n",
      "  inflating: ./titanic-dvc/data/train.csv  \r\n"
     ]
    }
   ],
   "source": [
    "!unzip './titanic-dvc/data/titanic.zip' -d './titanic-dvc/data'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see three new files (gender_submission.csv, test.csv and train.csv) in data directory now. We will not use gender_submission.csv, so let’s remove it along with Zip archive:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34m.\u001b[00m\r\n",
      "├── Data\\ Science\\ Workflows.ipynb\r\n",
      "├── Machine\\ Learning\\ Pipeline.ipynb\r\n",
      "├── README.md\r\n",
      "├── bigmart_ml_pipeline.py\r\n",
      "├── \u001b[01;34mdata\u001b[00m\r\n",
      "│   ├── SampleSubmission_Big_Mart_Sales_3.csv\r\n",
      "│   ├── gender_submission.csv\r\n",
      "│   ├── test.csv\r\n",
      "│   ├── test_Big_Mart_Sales_3.csv\r\n",
      "│   ├── train.csv\r\n",
      "│   └── train_Big_Mart_Sales_3.csv\r\n",
      "├── \u001b[01;34mresources\u001b[00m\r\n",
      "│   └── final_pipeline.webp\r\n",
      "└── \u001b[01;34mtitanic-dvc\u001b[00m\r\n",
      "    ├── README.md\r\n",
      "    ├── \u001b[01;34mdata\u001b[00m\r\n",
      "    │   ├── gender_submission.csv\r\n",
      "    │   ├── test.csv\r\n",
      "    │   ├── \u001b[01;31mtitanic.zip\u001b[00m\r\n",
      "    │   └── train.csv\r\n",
      "    ├── \u001b[01;34mfeatures\u001b[00m\r\n",
      "    ├── \u001b[01;34mpytitanic\u001b[00m\r\n",
      "    └── \u001b[01;34mresults\u001b[00m\r\n",
      "\r\n",
      "7 directories, 16 files\r\n"
     ]
    }
   ],
   "source": [
    "!tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm './titanic-dvc/data/gender_submission.csv' './titanic-dvc/data/titanic.zip'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34m.\u001b[00m\r\n",
      "├── Data\\ Science\\ Workflows.ipynb\r\n",
      "├── Machine\\ Learning\\ Pipeline.ipynb\r\n",
      "├── README.md\r\n",
      "├── bigmart_ml_pipeline.py\r\n",
      "├── \u001b[01;34mdata\u001b[00m\r\n",
      "│   ├── SampleSubmission_Big_Mart_Sales_3.csv\r\n",
      "│   ├── test_Big_Mart_Sales_3.csv\r\n",
      "│   └── train_Big_Mart_Sales_3.csv\r\n",
      "├── \u001b[01;34mresources\u001b[00m\r\n",
      "│   └── final_pipeline.webp\r\n",
      "└── \u001b[01;34mtitanic-dvc\u001b[00m\r\n",
      "    ├── README.md\r\n",
      "    ├── \u001b[01;34mdata\u001b[00m\r\n",
      "    │   ├── test.csv\r\n",
      "    │   └── train.csv\r\n",
      "    ├── \u001b[01;34mfeatures\u001b[00m\r\n",
      "    ├── \u001b[01;34mpytitanic\u001b[00m\r\n",
      "    └── \u001b[01;34mresults\u001b[00m\r\n",
      "\r\n",
      "7 directories, 11 files\r\n"
     ]
    }
   ],
   "source": [
    "!tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note, that we do not put data files under Git control, as their versioning will be handled by DVC. From now on, data files won’t be managed by Git directly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Managing data with DVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to initialize DVC for our project. To do this, launch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You can now commit the changes to git.\n",
      "\n",
      "\u001b[31m+---------------------------------------------------------------------+\n",
      "\u001b[39m\u001b[31m|\u001b[39m                                                                     \u001b[31m|\u001b[39m\n",
      "\u001b[31m|\u001b[39m        DVC has enabled anonymous aggregate usage analytics.         \u001b[31m|\u001b[39m\n",
      "\u001b[31m|\u001b[39m     Read the analytics documentation (and how to opt-out) here:     \u001b[31m|\u001b[39m\n",
      "\u001b[31m|\u001b[39m              \u001b[34mhttps://dvc.org/doc/user-guide/analytics\u001b[39m               \u001b[31m|\u001b[39m\n",
      "\u001b[31m|\u001b[39m                                                                     \u001b[31m|\u001b[39m\n",
      "\u001b[31m+---------------------------------------------------------------------+\n",
      "\u001b[39m\n",
      "\u001b[33mWhat's next?\u001b[39m\n",
      "\u001b[33m------------\u001b[39m\n",
      "- Check out the documentation: \u001b[34mhttps://dvc.org/doc\u001b[39m\n",
      "- Get help and share ideas: \u001b[34mhttps://dvc.org/chat\u001b[39m\n",
      "- Star us on GitHub: \u001b[34mhttps://github.com/iterative/dvc\u001b[39m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!dvc init"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Several things happen when DVC performs initialization. First, it creates `.dvc` directory to hold its own files needed for operation. `.dvc` directory is the same for DVC, as `.git` is for Git.\n",
    "\n",
    "Second, DVC instructs Git on how to handle newly created files. If you look at current status for Git (with `git status`), you’ll see, that DVC staged its files to commit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sur la branche master\n",
      "Votre branche est à jour avec 'origin/master'.\n",
      "\n",
      "Modifications qui seront validées :\n",
      "  (utilisez \"git restore --staged <fichier>...\" pour désindexer)\n",
      "\t\u001b[32mnouveau fichier : .dvc/.gitignore\u001b[m\n",
      "\t\u001b[32mnouveau fichier : .dvc/config\u001b[m\n",
      "\n",
      "Modifications qui ne seront pas validées :\n",
      "  (utilisez \"git add <fichier>...\" pour mettre à jour ce qui sera validé)\n",
      "  (utilisez \"git restore <fichier>...\" pour annuler les modifications dans le répertoire de travail)\n",
      "\t\u001b[31mmodifié :         .DS_Store\u001b[m\n",
      "\n",
      "Fichiers non suivis:\n",
      "  (utilisez \"git add <fichier>...\" pour inclure dans ce qui sera validé)\n",
      "\t\u001b[31m.ipynb_checkpoints/Data Science Workflows-checkpoint.ipynb\u001b[m\n",
      "\t\u001b[31mData Science Workflows.ipynb\u001b[m\n",
      "\t\u001b[31mtitanic-dvc/\u001b[m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!git status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`.dvc/.gitignore` file instructs Git to skip some DVC internal files from .dvc, \n",
    "\n",
    "while `.dvc/config` contains the newly created configuration for DVC, which is empty for now.\n",
    "\n",
    ">DVC tries to name commands in a familiar way. Most of the time, DVC command does exactly what you would expect it to do based on your Git experience.\n",
    "\n",
    ">Moreover, DVC is a pretty verbose tool and most of the commands output meaningful and useful messages, so that you can understand what’s going on and what to do next.\n",
    "\n",
    "Let’s commit the changes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[master a454c1a] DVC was initialized for the project.\r\n",
      " 2 files changed, 9 insertions(+)\r\n",
      " create mode 100644 .dvc/.gitignore\r\n",
      " create mode 100644 .dvc/config\r\n"
     ]
    }
   ],
   "source": [
    "!git commit -m \"DVC was initialized for the project.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to track the data files with DVC. To tell DVC about `data/train.csv` and `data/test.csv` we’ll use `dvc add`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100% Add|██████████████████████████████████|2.00/2.00 [00:02<00:00,  1.15s/file]\u001b[0m\n",
      "\n",
      "To track the changes with git, run:\n",
      "\n",
      "\tgit add titanic-dvc/data/train.csv.dvc titanic-dvc/data/test.csv.dvc titanic-dvc/data/.gitignore\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!dvc add './titanic-dvc/data/train.csv' './titanic-dvc/data/test.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s break this down. First, DVC creates yet another `.gitignore` file to exclude original data files from Git tracking.\n",
    "\n",
    "Second, something more important happens: DVC puts data files in its cache, and creates two metafiles (`data/train.csv.dvc` and `data/test.csv.dvc`) with the information about original data files.\n",
    "\n",
    "Metafiles follow YAML standard and have a specific set of attributes (use `cat data/train.csv.dvc` to look into the file):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34m./titanic-dvc\u001b[00m\r\n",
      "├── README.md\r\n",
      "├── \u001b[01;34mdata\u001b[00m\r\n",
      "│   ├── test.csv\r\n",
      "│   ├── test.csv.dvc\r\n",
      "│   ├── train.csv\r\n",
      "│   └── train.csv.dvc\r\n",
      "├── \u001b[01;34mfeatures\u001b[00m\r\n",
      "├── \u001b[01;34mpytitanic\u001b[00m\r\n",
      "└── \u001b[01;34mresults\u001b[00m\r\n",
      "\r\n",
      "4 directories, 5 files\r\n"
     ]
    }
   ],
   "source": [
    "!tree './titanic-dvc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "md5: ee3ee8f3eac58359eb30beaa0e56aa02\r\n",
      "outs:\r\n",
      "- md5: 61fdd54abdbf6a85b778e937122e1194\r\n",
      "  path: train.csv\r\n",
      "  cache: true\r\n",
      "  metric: false\r\n",
      "  persist: false\r\n"
     ]
    }
   ],
   "source": [
    "!cat './titanic-dvc/data/train.csv.dvc'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Top-level md5 attribute contains MD5 checksum for *.dvc file contents, while md5 attribute under outs contains the checksum of the data file itself.\n",
    "\n",
    ">You may notice, that md5sum utility calculates different values for both data file and *.dvc file. That’s ok, as DVC calculates MD5 on a transformed version of a file. For text files it changes EOL sequence from \\r\\n (which is the case for Titanic dataset files) to \\n.\n",
    "\n",
    "> For *.dvc file itself it’s even more elaborate: top-level MD5 attribute contains checksum not for the *.dvc file itself (think about this for a moment), but for properly encoded string representation of the contents, with some filtering applied.\n",
    "\n",
    "Now let’s look at the cache. It’s located by default in `.dvc/cache`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34m.dvc/cache/\u001b[00m\r\n",
      "├── \u001b[01;34m02\u001b[00m\r\n",
      "│   └── 9c9cd22461f6dbe8d9ab01def965c6\r\n",
      "└── \u001b[01;34m61\u001b[00m\r\n",
      "    └── fdd54abdbf6a85b778e937122e1194\r\n",
      "\r\n",
      "2 directories, 2 files\r\n"
     ]
    }
   ],
   "source": [
    "!tree .dvc/cache/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, DVC stores data files in the cache according to their MD5: first two symbols form directory name, while remaining ones are used as the cache file name.\n",
    "\n",
    "We can now commit DVC metafiles to Git:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/mac/Desktop/sideProjects/learnML/titanic-dvc\n"
     ]
    }
   ],
   "source": [
    "cd ./titanic-dvc/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git add data/.gitignore data/test.csv.dvc data/train.csv.dvc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[master c5e6acb] Original data files added\r\n",
      " 3 files changed, 16 insertions(+)\r\n",
      " create mode 100644 titanic-dvc/data/.gitignore\r\n",
      " create mode 100644 titanic-dvc/data/test.csv.dvc\r\n",
      " create mode 100644 titanic-dvc/data/train.csv.dvc\r\n"
     ]
    }
   ],
   "source": [
    "!git commit -m \"Original data files added\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**, that Git knows nothing about data files themselves, all the information needed to track them is stored in DVC files, while Git serves as an upper-level tool to track DVC itself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Moving data around in a controllable way"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the data files are now under DVC control, we can start using it. For example, if you accidentally deleted one of the data files, you can recreate it from cache with `dvc checkout`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm data/train.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34m.\u001b[00m\r\n",
      "├── README.md\r\n",
      "├── \u001b[01;34mdata\u001b[00m\r\n",
      "│   ├── test.csv\r\n",
      "│   ├── test.csv.dvc\r\n",
      "│   └── train.csv.dvc\r\n",
      "├── \u001b[01;34mfeatures\u001b[00m\r\n",
      "├── \u001b[01;34mpytitanic\u001b[00m\r\n",
      "└── \u001b[01;34mresults\u001b[00m\r\n",
      "\r\n",
      "4 directories, 4 files\r\n"
     ]
    }
   ],
   "source": [
    "!tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m+-------------------------------------------+\n",
      "\u001b[39m\u001b[33m|\u001b[39m                                           \u001b[33m|\u001b[39m\n",
      "\u001b[33m|\u001b[39m     Update available \u001b[31m0.77.3\u001b[39m -> \u001b[32m0.83.0\u001b[39m     \u001b[33m|\u001b[39m\n",
      "\u001b[33m|\u001b[39m       Run \u001b[33mpip\u001b[39m install dvc \u001b[34m--upgrade\u001b[39m       \u001b[33m|\u001b[39m\n",
      "\u001b[33m|\u001b[39m                                           \u001b[33m|\u001b[39m\n",
      "\u001b[33m+-------------------------------------------+\n",
      "\u001b[39m\n",
      "\u001b[0m                                                                            "
     ]
    }
   ],
   "source": [
    "!dvc checkout data/train.csv.dvc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34m.\u001b[00m\r\n",
      "├── README.md\r\n",
      "├── \u001b[01;34mdata\u001b[00m\r\n",
      "│   ├── test.csv\r\n",
      "│   ├── test.csv.dvc\r\n",
      "│   ├── train.csv\r\n",
      "│   └── train.csv.dvc\r\n",
      "├── \u001b[01;34mfeatures\u001b[00m\r\n",
      "├── \u001b[01;34mpytitanic\u001b[00m\r\n",
      "└── \u001b[01;34mresults\u001b[00m\r\n",
      "\r\n",
      "4 directories, 5 files\r\n"
     ]
    }
   ],
   "source": [
    "!tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`dvc checkout` looks into the target file (data/train.csv.dvc in this case) and retrieves the corresponding version from the cache. This is, of course, a simple example, but it illustrates the pattern.\n",
    "\n",
    "A more elaborate example would include **remote storage**. DVC can store files outside the working directory. This allows to easily share files using DVC tools. DVC allows using a local directory, AWS S3, Azure, and other destinations as remotes.\n",
    "\n",
    "Let’s create local remote storage for the data files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/mac/Desktop/sideProjects/learnML/titanic-dvc'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir ../titanic-remote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting 'localremote' as a default remote.\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!dvc remote add -d localremote ../titanic-remote/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now push the data to the newly created remote:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m+-------------------------------------------+\n",
      "\u001b[39m\u001b[33m|\u001b[39m                                           \u001b[33m|\u001b[39m\n",
      "\u001b[33m|\u001b[39m     Update available \u001b[31m0.77.3\u001b[39m -> \u001b[32m0.83.0\u001b[39m     \u001b[33m|\u001b[39m\n",
      "\u001b[33m|\u001b[39m       Run \u001b[33mpip\u001b[39m install dvc \u001b[34m--upgrade\u001b[39m       \u001b[33m|\u001b[39m\n",
      "\u001b[33m|\u001b[39m                                           \u001b[33m|\u001b[39m\n",
      "\u001b[33m+-------------------------------------------+\n",
      "\u001b[39m\n",
      "\u001b[0m                                                                            "
     ]
    }
   ],
   "source": [
    "!dvc push data/train.csv.dvc data/test.csv.dvc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remotes are structured similarly to local cache:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34m../titanic-remote/\u001b[00m\r\n",
      "├── \u001b[01;34m02\u001b[00m\r\n",
      "│   └── 9c9cd22461f6dbe8d9ab01def965c6\r\n",
      "└── \u001b[01;34m61\u001b[00m\r\n",
      "    └── fdd54abdbf6a85b778e937122e1194\r\n",
      "\r\n",
      "2 directories, 2 files\r\n"
     ]
    }
   ],
   "source": [
    "!tree ../titanic-remote/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By itself, remotes are of limited usefulness. However, they become crucial when you work in a team. Let’s illustrate this by creating a clone of our current repository:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/mac/Desktop/sideProjects/learnML\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: le dépôt 'titanic-dvc' n'existe pas\r\n"
     ]
    }
   ],
   "source": [
    "# Transform titanic-dvc into a git repo if you didn't before\n",
    "!git clone titanic-dvc titanic-dvc-copy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/mac/Desktop/sideProjects/learnML/titanic-dvc-copy\n"
     ]
    }
   ],
   "source": [
    "cd titanic-dvc-copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34m.\u001b[00m\r\n",
      "├── README.md\r\n",
      "└── \u001b[01;34mdata\u001b[00m\r\n",
      "    ├── test.csv.dvc\r\n",
      "    └── train.csv.dvc\r\n",
      "\r\n",
      "1 directory, 3 files\r\n"
     ]
    }
   ],
   "source": [
    "!tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Original data files are not there, but DVC metafiles are, as they are tracked by Git. This allows us to easily fetch data files from existing remote storage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting 'localremote' as a default remote.\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!dvc remote add -d localremote --local ../titanic-remote/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m+-------------------------------------------+\n",
      "\u001b[39m\u001b[33m|\u001b[39m                                           \u001b[33m|\u001b[39m\n",
      "\u001b[33m|\u001b[39m     Update available \u001b[31m0.77.3\u001b[39m -> \u001b[32m0.83.0\u001b[39m     \u001b[33m|\u001b[39m\n",
      "\u001b[33m|\u001b[39m       Run \u001b[33mpip\u001b[39m install dvc \u001b[34m--upgrade\u001b[39m       \u001b[33m|\u001b[39m\n",
      "\u001b[33m|\u001b[39m                                           \u001b[33m|\u001b[39m\n",
      "\u001b[33m+-------------------------------------------+\n",
      "\u001b[39m\n",
      "Everything is up to date.                                                       \n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!dvc pull data/train.csv.dvc data/test.csv.dvc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34m.\u001b[00m\r\n",
      "├── README.md\r\n",
      "└── \u001b[01;34mdata\u001b[00m\r\n",
      "    ├── test.csv\r\n",
      "    ├── test.csv.dvc\r\n",
      "    ├── train.csv\r\n",
      "    └── train.csv.dvc\r\n",
      "\r\n",
      "1 directory, 5 files\r\n"
     ]
    }
   ],
   "source": [
    "!tree ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Information about remotes is stored in DVC config files (.dvc/config). Let’s get back to the original repository and look at how the configuration file changed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['remote \"localremote\"']\r\n",
      "url = ../titanic-remote\r\n",
      "[core]\r\n",
      "remote = localremote\r\n"
     ]
    }
   ],
   "source": [
    "!cat ../.dvc/config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now commit the changes to DVC config:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/mac/Desktop/sideProjects/learnML\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git add .dvc/config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[master bc5ad3e] add local remote\r\n",
      " 1 file changed, 4 insertions(+)\r\n"
     ]
    }
   ],
   "source": [
    "!git commit -m \"add local remote\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note**, that the newly created remote is local and thus its configuration may be kept outside of Git. DVC allows user to have several types of configuration, and the one called local is excluded from Git tracking. To use local configuration when creating remotes, just add `— local` option to `dvc remote add` command. We added this option for titanic-dvc-copy repository for illustration.\n",
    "\n",
    "> The same `— local` option should be used when creating **cloud-backed remotes**, as you’ll need to add credentials to access AWS S3, Azure or GCP remotes and it’s not recommended to have them in Git."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Versioning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, we used DVC to only add files to cache and remote storage. This is only a part of the story. More importantly, data files can be versioned. Of course, Titanic dataset would not change, but real datasets can change over time.\n",
    "\n",
    "We will simulate changes in data by just renaming Name column to FullName in both data files. This is enough for our purposes, as DVC doesn’t care, what actually changed, it just tracks the changes.\n",
    "\n",
    "For convenience, let’s tag the latest Git commit so that we can easily checkout files from it without messing with hashes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git tag base-dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now add edited data files to DVC:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m+-------------------------------------------+\n",
      "\u001b[39m\u001b[33m|\u001b[39m                                           \u001b[33m|\u001b[39m\n",
      "\u001b[33m|\u001b[39m     Update available \u001b[31m0.77.3\u001b[39m -> \u001b[32m0.83.0\u001b[39m     \u001b[33m|\u001b[39m\n",
      "\u001b[33m|\u001b[39m       Run \u001b[33mpip\u001b[39m install dvc \u001b[34m--upgrade\u001b[39m       \u001b[33m|\u001b[39m\n",
      "\u001b[33m|\u001b[39m                                           \u001b[33m|\u001b[39m\n",
      "\u001b[33m+-------------------------------------------+\n",
      "\u001b[39m\n",
      "Stage is cached, skipping.                                                      \n",
      "100% Add|██████████████████████████████████|1.00/1.00 [00:01<00:00,  1.65s/file]\n",
      "\n",
      "To track the changes with git, run:\n",
      "\n",
      "\tgit add data/test.csv.dvc\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!dvc add data/train.csv data/test.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git add data/test.csv.dvc data/train.csv.dvc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[master 69340fa] rename dataset columns\n",
      " 1 file changed, 7 insertions(+)\n",
      " create mode 100644 data/test.csv.dvc\n"
     ]
    }
   ],
   "source": [
    "!git commit -m \"rename dataset columns\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: l'étiquette 'renamed-dataset' existe déjà\r\n"
     ]
    }
   ],
   "source": [
    "!git tag renamed-dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m+-------------------------------------------+\n",
      "\u001b[39m\u001b[33m|\u001b[39m                                           \u001b[33m|\u001b[39m\n",
      "\u001b[33m|\u001b[39m     Update available \u001b[31m0.77.3\u001b[39m -> \u001b[32m0.83.0\u001b[39m     \u001b[33m|\u001b[39m\n",
      "\u001b[33m|\u001b[39m       Run \u001b[33mpip\u001b[39m install dvc \u001b[34m--upgrade\u001b[39m       \u001b[33m|\u001b[39m\n",
      "\u001b[33m|\u001b[39m                                           \u001b[33m|\u001b[39m\n",
      "\u001b[33m+-------------------------------------------+\n",
      "\u001b[39m\n",
      "\u001b[0m                                                                            "
     ]
    }
   ],
   "source": [
    "!dvc push data/train.csv.dvc data/test.csv.dvc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34m../.dvc/cache\u001b[00m\r\n",
      "├── \u001b[01;34m02\u001b[00m\r\n",
      "│   └── 9c9cd22461f6dbe8d9ab01def965c6\r\n",
      "├── \u001b[01;34m61\u001b[00m\r\n",
      "│   └── fdd54abdbf6a85b778e937122e1194\r\n",
      "├── \u001b[01;34m8b\u001b[00m\r\n",
      "│   └── 686a9e84994cb693999608b9201619\r\n",
      "└── \u001b[01;34mee\u001b[00m\r\n",
      "    └── f968ede3c2968e56e886c6b94d265a\r\n",
      "\r\n",
      "4 directories, 4 files\r\n"
     ]
    }
   ],
   "source": [
    "!tree ../.dvc/cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the moment, data files in the working directory contain renamed columns, and DVC added them to cache (check this with `tree .dvc/cache`) and we pushed it to the local remote. So far, all the changes were propagated to all locations.\n",
    "\n",
    "Lets’ assume, that you want to get the original version of the data. We intentionally tagged the corresponding commit, and now can easily checkout DVC metafiles for that version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 chemins mis à jour depuis 3792fb7\r\n"
     ]
    }
   ],
   "source": [
    "!git checkout base-dataset data/train.csv.dvc data/test.csv.dvc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the metafiles for `base-dataset`, we can easily get the original version of the data files from cache or remote storage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m+-------------------------------------------+\n",
      "\u001b[39m\u001b[33m|\u001b[39m                                           \u001b[33m|\u001b[39m\n",
      "\u001b[33m|\u001b[39m     Update available \u001b[31m0.77.3\u001b[39m -> \u001b[32m0.83.0\u001b[39m     \u001b[33m|\u001b[39m\n",
      "\u001b[33m|\u001b[39m       Run \u001b[33mpip\u001b[39m install dvc \u001b[34m--upgrade\u001b[39m       \u001b[33m|\u001b[39m\n",
      "\u001b[33m|\u001b[39m                                           \u001b[33m|\u001b[39m\n",
      "\u001b[33m+-------------------------------------------+\n",
      "\u001b[39m\n",
      "\u001b[33mWARNING\u001b[39m: data 'data/test.csv' exists. Removing before checkout.       \n",
      "\u001b[33mWARNING\u001b[39m: data 'data/train.csv' exists. Removing before checkout.      \n",
      "\u001b[0m                                                                            "
     ]
    }
   ],
   "source": [
    "!dvc checkout data/train.csv.dvc data/test.csv.dvc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you checkout the data files, you will see, that column is named Name, as it was in the original files. To revert data files to the current form with FullName instead of Name, just reset corresponding metafiles to `Git HEAD`and perform `dvc checkout` again.\n",
    "\n",
    "> Note, that with DVC we effectively version control metafiles. All the tracking of actual data files is performed by DVC based on information in metafiles.\n",
    "\n",
    "As you can see, DVC is convenient and simple enough to be used for data versioning. It allows to easily record the state of the data, and switch between different versions (with some help from Git).\n",
    "\n",
    "This reduces the mess and helps to keep data coherent across teammates and locations. However, DVC can do more: it can track calculations and results, allowing to recreate any previous result without a lot of trouble."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Managing calculations with DVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datascience_devenv",
   "language": "python",
   "name": "datascience_devenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

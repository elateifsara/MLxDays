{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating reproducible data science workflows with DVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tutorial [link](https://medium.com/y-data-stories/creating-reproducible-data-science-workflows-with-dvc-3bf058e9797b)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Despite being implemented in software, development of data science and machine learning projects is dramatically different from general-purpose software development: for example, DS is primarily experiment-driven and has a much higher level of intrinsic unpredictability.\n",
    "\n",
    "Data science and machine learning are very different. Instead of features to implement, in ML we have ideas to try out, without any guarantee of succeeding. Before long, you may test a few dozen hypotheses, often substantially varying in their complexity and the implementation required. Most of them will fail, some may look promising, and ultimately, some complicated combinations of them may succeed.\n",
    "\n",
    "Eventually, you or your colleague will try to reproduce one of those models, the one which seems to be the best. You may find out that you cannot easily reconstruct how exactly that model was created. It may turn out that the training parameters are buried somewhere in a notebook, overwritten by subsequent experiments and multiple Git commits in several branches. In addition, perhaps the training/cross-validation split was performed without setting a random seed. And if you’re still not worried enough, the cherry on top is that when examined, it turns out the current features look nothing like what they were when the model was created. Too bad."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Science processes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some teams use the variants of [Coockiecutter Data Science](https://drivendata.github.io/cookiecutter-data-science/), others follow the approach outlined in [Guerilla Analytics](http://guerrilla-analytics.net/the-principles/). Both address the same problems, both offer powerful tools and we recommend you to try them out.  \n",
    "\n",
    "Chances are, you or your team already use something similar. But regardless of which approach you use to write reproducible data science code, you need tooling. \n",
    "\n",
    "The bare minimum requirements are the following:\n",
    "- a way to **version-control the data**, especially intermediate artefacts like pre-computed features and models,\n",
    "- being able to **pass data files inside the team** in a controllable and trackable way,\n",
    "- a tool to **reproduce any artefacts or results**, in a simple and automated way, regardless of how long ago they were originally created."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** Other terms like documents, deliverables, or work products are widely used in software development communities instead of the term **artefact**. \n",
    "\n",
    "More: https://www.quora.com/What-does-the-term-artefact-mean-in-software-engineering-or-programming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial we will explore, how DVC implements all of the processes we’ve outlined and makes reproducible data science easier. DVC is open-source and attempts to be a Git for machine learning, while working closely together with Git itself.  \n",
    "\n",
    "**DVC** is actually quite a large tool, but the most common operations are simple enough, in the same way that basic Git commands are easy to learn and incorporate into daily practice.  \n",
    "\n",
    "DVC is not the only tool for the job. It works best for small to middle-sized projects and solves the problem without adding too much complexity. However, depending on your needs, project size and deployment considerations you may find Kedro or other tools more suitable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Versioning the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s first define what **data version control** is:\n",
    "- state of any data file, whether original one or derived, must be recorded,\n",
    "- there must be a tool to switch between different versions of data files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the following scenario: the training data comes from a relational database and is stored as a CSV file. Once in a while, you want to update the dataset with recent records from the database. Each time you do so, you record the state of the dataset. If you have a way to *switch to any of the previous versions and back* — congratulations, your data is version controlled.  \n",
    "\n",
    "Git is not suitable for this, as it was not designed to serve large or binary files, while extensions like Git LFS are general-purpose and can be used for data version control only with some limitations and inconvenience. DVC offers a more flexible approach.  \n",
    "\n",
    "To illustrate it we will use [Titanic dataset](https://www.kaggle.com/c/titanic) from Kaggle and build a simple model and a submission file. With this miniature data science project, we will see, how **DVC helps to ensure data lineage and reproducibility**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install DVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install dvc\n",
    "# Or\n",
    "!conda install -c conda-forge dvc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> *Note, that you can configure DVC to use external storage to hold and exchange data, and in that case you’ll also need to install additional dependencies. For example, if you plan to use Amazon Web Services S3, you need to install boto and some other packages with*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install dvc[s3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Install [AWS CLI v2](https://docs.aws.amazon.com/cli/latest/userguide/install-cliv2.html)\n",
    "- Configure [AWS](https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-configure.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project Layout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our project starts with the Titanic dataset, which contains two files (one for training and one for testing), and the project structure.  \n",
    "\n",
    "It may be tempting to have all the data files and code in the same directory for the project this small.  \n",
    "\n",
    "However, it’s strategically wiser to stick to the same project structure for any project, regardless of its size. A disciplined approach to project structure and operations saves a lot of headache and time along the road.\n",
    "\n",
    "Let’s create a skeleton for our project:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir titanic-dvc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd titanic-dvc/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: ./titanic-dvc/data: File exists\r\n",
      "mkdir: ./titanic-dvc/features: File exists\r\n",
      "mkdir: ./titanic-dvc/results: File exists\r\n",
      "mkdir: ./titanic-dvc/pytitanic: File exists\r\n"
     ]
    }
   ],
   "source": [
    "!mkdir './titanic-dvc/'{data,features,results,pytitanic}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "!touch './titanic-dvc/README.md'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34m./titanic-dvc/\u001b[00m\r\n",
      "├── README.md\r\n",
      "├── \u001b[01;34mdata\u001b[00m\r\n",
      "├── \u001b[01;34mfeatures\u001b[00m\r\n",
      "├── \u001b[01;34mpytitanic\u001b[00m\r\n",
      "└── \u001b[01;34mresults\u001b[00m\r\n",
      "\r\n",
      "4 directories, 1 file\r\n"
     ]
    }
   ],
   "source": [
    "!tree './titanic-dvc/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The original data goes in the `data` directory. Although we have only two original data files, for larger projects there may be tens, thousands or even millions of data files, so it’s reasonable to have a separate directory for them.\n",
    "\n",
    "All derived features and intermediate data files go to `features` directory. Results (for example, trained models and submission files) will live in `results` directory.\n",
    "\n",
    "`pytitanic` directory will contain Python code for the project: scripts, modules, packages, etc. Additionally, you may have `notebooks` directory, and directories for code in other languages (for example, in R or Julia).\n",
    "\n",
    "We will keep `README.md` empty out of simplicity, although we still create it for the sake of the general procedure.\n",
    "\n",
    "To finish the project setup, we need to add data files and initialize Git repository and DVC. Given that you have already downloaded the data as a Zip archive in data directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  ./titanic-dvc/data/titanic.zip\r\n",
      "  inflating: ./titanic-dvc/data/gender_submission.csv  \r\n",
      "  inflating: ./titanic-dvc/data/test.csv  \r\n",
      "  inflating: ./titanic-dvc/data/train.csv  \r\n"
     ]
    }
   ],
   "source": [
    "!unzip './titanic-dvc/data/titanic.zip' -d './titanic-dvc/data'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see three new files (gender_submission.csv, test.csv and train.csv) in data directory now. We will not use gender_submission.csv, so let’s remove it along with Zip archive:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34m.\u001b[00m\r\n",
      "├── Data\\ Science\\ Workflows.ipynb\r\n",
      "├── Machine\\ Learning\\ Pipeline.ipynb\r\n",
      "├── README.md\r\n",
      "├── bigmart_ml_pipeline.py\r\n",
      "├── \u001b[01;34mdata\u001b[00m\r\n",
      "│   ├── SampleSubmission_Big_Mart_Sales_3.csv\r\n",
      "│   ├── gender_submission.csv\r\n",
      "│   ├── test.csv\r\n",
      "│   ├── test_Big_Mart_Sales_3.csv\r\n",
      "│   ├── train.csv\r\n",
      "│   └── train_Big_Mart_Sales_3.csv\r\n",
      "├── \u001b[01;34mresources\u001b[00m\r\n",
      "│   └── final_pipeline.webp\r\n",
      "└── \u001b[01;34mtitanic-dvc\u001b[00m\r\n",
      "    ├── README.md\r\n",
      "    ├── \u001b[01;34mdata\u001b[00m\r\n",
      "    │   ├── gender_submission.csv\r\n",
      "    │   ├── test.csv\r\n",
      "    │   ├── \u001b[01;31mtitanic.zip\u001b[00m\r\n",
      "    │   └── train.csv\r\n",
      "    ├── \u001b[01;34mfeatures\u001b[00m\r\n",
      "    ├── \u001b[01;34mpytitanic\u001b[00m\r\n",
      "    └── \u001b[01;34mresults\u001b[00m\r\n",
      "\r\n",
      "7 directories, 16 files\r\n"
     ]
    }
   ],
   "source": [
    "!tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm './titanic-dvc/data/gender_submission.csv' './titanic-dvc/data/titanic.zip'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34m.\u001b[00m\r\n",
      "├── Data\\ Science\\ Workflows.ipynb\r\n",
      "├── Machine\\ Learning\\ Pipeline.ipynb\r\n",
      "├── README.md\r\n",
      "├── bigmart_ml_pipeline.py\r\n",
      "├── \u001b[01;34mdata\u001b[00m\r\n",
      "│   ├── SampleSubmission_Big_Mart_Sales_3.csv\r\n",
      "│   ├── test_Big_Mart_Sales_3.csv\r\n",
      "│   └── train_Big_Mart_Sales_3.csv\r\n",
      "├── \u001b[01;34mresources\u001b[00m\r\n",
      "│   └── final_pipeline.webp\r\n",
      "└── \u001b[01;34mtitanic-dvc\u001b[00m\r\n",
      "    ├── README.md\r\n",
      "    ├── \u001b[01;34mdata\u001b[00m\r\n",
      "    │   ├── test.csv\r\n",
      "    │   └── train.csv\r\n",
      "    ├── \u001b[01;34mfeatures\u001b[00m\r\n",
      "    ├── \u001b[01;34mpytitanic\u001b[00m\r\n",
      "    └── \u001b[01;34mresults\u001b[00m\r\n",
      "\r\n",
      "7 directories, 11 files\r\n"
     ]
    }
   ],
   "source": [
    "!tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note, that we do not put data files under Git control, as their versioning will be handled by DVC. From now on, data files won’t be managed by Git directly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Managing data with DVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to initialize DVC for our project. To do this, launch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You can now commit the changes to git.\n",
      "\n",
      "\u001b[31m+---------------------------------------------------------------------+\n",
      "\u001b[39m\u001b[31m|\u001b[39m                                                                     \u001b[31m|\u001b[39m\n",
      "\u001b[31m|\u001b[39m        DVC has enabled anonymous aggregate usage analytics.         \u001b[31m|\u001b[39m\n",
      "\u001b[31m|\u001b[39m     Read the analytics documentation (and how to opt-out) here:     \u001b[31m|\u001b[39m\n",
      "\u001b[31m|\u001b[39m              \u001b[34mhttps://dvc.org/doc/user-guide/analytics\u001b[39m               \u001b[31m|\u001b[39m\n",
      "\u001b[31m|\u001b[39m                                                                     \u001b[31m|\u001b[39m\n",
      "\u001b[31m+---------------------------------------------------------------------+\n",
      "\u001b[39m\n",
      "\u001b[33mWhat's next?\u001b[39m\n",
      "\u001b[33m------------\u001b[39m\n",
      "- Check out the documentation: \u001b[34mhttps://dvc.org/doc\u001b[39m\n",
      "- Get help and share ideas: \u001b[34mhttps://dvc.org/chat\u001b[39m\n",
      "- Star us on GitHub: \u001b[34mhttps://github.com/iterative/dvc\u001b[39m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!dvc init"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Several things happen when DVC performs initialization. First, it creates `.dvc` directory to hold its own files needed for operation. `.dvc` directory is the same for DVC, as `.git` is for Git.\n",
    "\n",
    "Second, DVC instructs Git on how to handle newly created files. If you look at current status for Git (with `git status`), you’ll see, that DVC staged its files to commit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sur la branche master\n",
      "Votre branche est à jour avec 'origin/master'.\n",
      "\n",
      "Modifications qui seront validées :\n",
      "  (utilisez \"git restore --staged <fichier>...\" pour désindexer)\n",
      "\t\u001b[32mnouveau fichier : .dvc/.gitignore\u001b[m\n",
      "\t\u001b[32mnouveau fichier : .dvc/config\u001b[m\n",
      "\n",
      "Modifications qui ne seront pas validées :\n",
      "  (utilisez \"git add <fichier>...\" pour mettre à jour ce qui sera validé)\n",
      "  (utilisez \"git restore <fichier>...\" pour annuler les modifications dans le répertoire de travail)\n",
      "\t\u001b[31mmodifié :         .DS_Store\u001b[m\n",
      "\n",
      "Fichiers non suivis:\n",
      "  (utilisez \"git add <fichier>...\" pour inclure dans ce qui sera validé)\n",
      "\t\u001b[31m.ipynb_checkpoints/Data Science Workflows-checkpoint.ipynb\u001b[m\n",
      "\t\u001b[31mData Science Workflows.ipynb\u001b[m\n",
      "\t\u001b[31mtitanic-dvc/\u001b[m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!git status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`.dvc/.gitignore` file instructs Git to skip some DVC internal files from .dvc, \n",
    "\n",
    "while `.dvc/config` contains the newly created configuration for DVC, which is empty for now.\n",
    "\n",
    ">DVC tries to name commands in a familiar way. Most of the time, DVC command does exactly what you would expect it to do based on your Git experience.\n",
    "\n",
    ">Moreover, DVC is a pretty verbose tool and most of the commands output meaningful and useful messages, so that you can understand what’s going on and what to do next.\n",
    "\n",
    "Let’s commit the changes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[master a454c1a] DVC was initialized for the project.\r\n",
      " 2 files changed, 9 insertions(+)\r\n",
      " create mode 100644 .dvc/.gitignore\r\n",
      " create mode 100644 .dvc/config\r\n"
     ]
    }
   ],
   "source": [
    "!git commit -m \"DVC was initialized for the project.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to track the data files with DVC. To tell DVC about `data/train.csv` and `data/test.csv` we’ll use `dvc add`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100% Add|██████████████████████████████████|2.00/2.00 [00:02<00:00,  1.15s/file]\u001b[0m\n",
      "\n",
      "To track the changes with git, run:\n",
      "\n",
      "\tgit add titanic-dvc/data/train.csv.dvc titanic-dvc/data/test.csv.dvc titanic-dvc/data/.gitignore\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!dvc add './titanic-dvc/data/train.csv' './titanic-dvc/data/test.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s break this down. First, DVC creates yet another `.gitignore` file to exclude original data files from Git tracking.\n",
    "\n",
    "Second, something more important happens: DVC puts data files in its cache, and creates two metafiles (`data/train.csv.dvc` and `data/test.csv.dvc`) with the information about original data files.\n",
    "\n",
    "Metafiles follow YAML standard and have a specific set of attributes (use `cat data/train.csv.dvc` to look into the file):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34m./titanic-dvc\u001b[00m\r\n",
      "├── README.md\r\n",
      "├── \u001b[01;34mdata\u001b[00m\r\n",
      "│   ├── test.csv\r\n",
      "│   ├── test.csv.dvc\r\n",
      "│   ├── train.csv\r\n",
      "│   └── train.csv.dvc\r\n",
      "├── \u001b[01;34mfeatures\u001b[00m\r\n",
      "├── \u001b[01;34mpytitanic\u001b[00m\r\n",
      "└── \u001b[01;34mresults\u001b[00m\r\n",
      "\r\n",
      "4 directories, 5 files\r\n"
     ]
    }
   ],
   "source": [
    "!tree './titanic-dvc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "md5: ee3ee8f3eac58359eb30beaa0e56aa02\r\n",
      "outs:\r\n",
      "- md5: 61fdd54abdbf6a85b778e937122e1194\r\n",
      "  path: train.csv\r\n",
      "  cache: true\r\n",
      "  metric: false\r\n",
      "  persist: false\r\n"
     ]
    }
   ],
   "source": [
    "!cat './titanic-dvc/data/train.csv.dvc'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Top-level md5 attribute contains MD5 checksum for *.dvc file contents, while md5 attribute under outs contains the checksum of the data file itself.\n",
    "\n",
    ">You may notice, that md5sum utility calculates different values for both data file and *.dvc file. That’s ok, as DVC calculates MD5 on a transformed version of a file. For text files it changes EOL sequence from \\r\\n (which is the case for Titanic dataset files) to \\n.\n",
    "\n",
    "> For *.dvc file itself it’s even more elaborate: top-level MD5 attribute contains checksum not for the *.dvc file itself (think about this for a moment), but for properly encoded string representation of the contents, with some filtering applied.\n",
    "\n",
    "Now let’s look at the cache. It’s located by default in `.dvc/cache`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34m.dvc/cache/\u001b[00m\r\n",
      "├── \u001b[01;34m02\u001b[00m\r\n",
      "│   └── 9c9cd22461f6dbe8d9ab01def965c6\r\n",
      "└── \u001b[01;34m61\u001b[00m\r\n",
      "    └── fdd54abdbf6a85b778e937122e1194\r\n",
      "\r\n",
      "2 directories, 2 files\r\n"
     ]
    }
   ],
   "source": [
    "!tree .dvc/cache/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, DVC stores data files in the cache according to their MD5: first two symbols form directory name, while remaining ones are used as the cache file name.\n",
    "\n",
    "We can now commit DVC metafiles to Git:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/mac/Desktop/sideProjects/learnML/titanic-dvc\n"
     ]
    }
   ],
   "source": [
    "cd ./titanic-dvc/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git add data/.gitignore data/test.csv.dvc data/train.csv.dvc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[master c5e6acb] Original data files added\r\n",
      " 3 files changed, 16 insertions(+)\r\n",
      " create mode 100644 titanic-dvc/data/.gitignore\r\n",
      " create mode 100644 titanic-dvc/data/test.csv.dvc\r\n",
      " create mode 100644 titanic-dvc/data/train.csv.dvc\r\n"
     ]
    }
   ],
   "source": [
    "!git commit -m \"Original data files added\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**, that Git knows nothing about data files themselves, all the information needed to track them is stored in DVC files, while Git serves as an upper-level tool to track DVC itself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Moving data around in a controllable way"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the data files are now under DVC control, we can start using it. For example, if you accidentally deleted one of the data files, you can recreate it from cache with `dvc checkout`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm data/train.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34m.\u001b[00m\r\n",
      "├── README.md\r\n",
      "├── \u001b[01;34mdata\u001b[00m\r\n",
      "│   ├── test.csv\r\n",
      "│   ├── test.csv.dvc\r\n",
      "│   └── train.csv.dvc\r\n",
      "├── \u001b[01;34mfeatures\u001b[00m\r\n",
      "├── \u001b[01;34mpytitanic\u001b[00m\r\n",
      "└── \u001b[01;34mresults\u001b[00m\r\n",
      "\r\n",
      "4 directories, 4 files\r\n"
     ]
    }
   ],
   "source": [
    "!tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m+-------------------------------------------+\n",
      "\u001b[39m\u001b[33m|\u001b[39m                                           \u001b[33m|\u001b[39m\n",
      "\u001b[33m|\u001b[39m     Update available \u001b[31m0.77.3\u001b[39m -> \u001b[32m0.83.0\u001b[39m     \u001b[33m|\u001b[39m\n",
      "\u001b[33m|\u001b[39m       Run \u001b[33mpip\u001b[39m install dvc \u001b[34m--upgrade\u001b[39m       \u001b[33m|\u001b[39m\n",
      "\u001b[33m|\u001b[39m                                           \u001b[33m|\u001b[39m\n",
      "\u001b[33m+-------------------------------------------+\n",
      "\u001b[39m\n",
      "\u001b[0m                                                                            "
     ]
    }
   ],
   "source": [
    "!dvc checkout data/train.csv.dvc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34m.\u001b[00m\r\n",
      "├── README.md\r\n",
      "├── \u001b[01;34mdata\u001b[00m\r\n",
      "│   ├── test.csv\r\n",
      "│   ├── test.csv.dvc\r\n",
      "│   ├── train.csv\r\n",
      "│   └── train.csv.dvc\r\n",
      "├── \u001b[01;34mfeatures\u001b[00m\r\n",
      "├── \u001b[01;34mpytitanic\u001b[00m\r\n",
      "└── \u001b[01;34mresults\u001b[00m\r\n",
      "\r\n",
      "4 directories, 5 files\r\n"
     ]
    }
   ],
   "source": [
    "!tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`dvc checkout` looks into the target file (data/train.csv.dvc in this case) and retrieves the corresponding version from the cache. This is, of course, a simple example, but it illustrates the pattern.\n",
    "\n",
    "A more elaborate example would include **remote storage**. DVC can store files outside the working directory. This allows to easily share files using DVC tools. DVC allows using a local directory, AWS S3, Azure, and other destinations as remotes.\n",
    "\n",
    "Let’s create local remote storage for the data files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/mac/Desktop/sideProjects/learnML/titanic-dvc'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir ../titanic-remote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting 'localremote' as a default remote.\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!dvc remote add -d localremote ../titanic-remote/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now push the data to the newly created remote:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m+-------------------------------------------+\n",
      "\u001b[39m\u001b[33m|\u001b[39m                                           \u001b[33m|\u001b[39m\n",
      "\u001b[33m|\u001b[39m     Update available \u001b[31m0.77.3\u001b[39m -> \u001b[32m0.83.0\u001b[39m     \u001b[33m|\u001b[39m\n",
      "\u001b[33m|\u001b[39m       Run \u001b[33mpip\u001b[39m install dvc \u001b[34m--upgrade\u001b[39m       \u001b[33m|\u001b[39m\n",
      "\u001b[33m|\u001b[39m                                           \u001b[33m|\u001b[39m\n",
      "\u001b[33m+-------------------------------------------+\n",
      "\u001b[39m\n",
      "\u001b[0m                                                                            "
     ]
    }
   ],
   "source": [
    "!dvc push data/train.csv.dvc data/test.csv.dvc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remotes are structured similarly to local cache:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34m../titanic-remote/\u001b[00m\r\n",
      "├── \u001b[01;34m02\u001b[00m\r\n",
      "│   └── 9c9cd22461f6dbe8d9ab01def965c6\r\n",
      "└── \u001b[01;34m61\u001b[00m\r\n",
      "    └── fdd54abdbf6a85b778e937122e1194\r\n",
      "\r\n",
      "2 directories, 2 files\r\n"
     ]
    }
   ],
   "source": [
    "!tree ../titanic-remote/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By itself, remotes are of limited usefulness. However, they become crucial when you work in a team. Let’s illustrate this by creating a clone of our current repository:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/mac/Desktop/sideProjects/learnML\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: le dépôt 'titanic-dvc' n'existe pas\r\n"
     ]
    }
   ],
   "source": [
    "# Transform titanic-dvc into a git repo if you didn't before\n",
    "!git clone titanic-dvc titanic-dvc-copy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/mac/Desktop/sideProjects/learnML/titanic-dvc-copy\n"
     ]
    }
   ],
   "source": [
    "cd titanic-dvc-copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34m.\u001b[00m\r\n",
      "├── README.md\r\n",
      "└── \u001b[01;34mdata\u001b[00m\r\n",
      "    ├── test.csv.dvc\r\n",
      "    └── train.csv.dvc\r\n",
      "\r\n",
      "1 directory, 3 files\r\n"
     ]
    }
   ],
   "source": [
    "!tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Original data files are not there, but DVC metafiles are, as they are tracked by Git. This allows us to easily fetch data files from existing remote storage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting 'localremote' as a default remote.\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!dvc remote add -d localremote --local ../titanic-remote/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m+-------------------------------------------+\n",
      "\u001b[39m\u001b[33m|\u001b[39m                                           \u001b[33m|\u001b[39m\n",
      "\u001b[33m|\u001b[39m     Update available \u001b[31m0.77.3\u001b[39m -> \u001b[32m0.83.0\u001b[39m     \u001b[33m|\u001b[39m\n",
      "\u001b[33m|\u001b[39m       Run \u001b[33mpip\u001b[39m install dvc \u001b[34m--upgrade\u001b[39m       \u001b[33m|\u001b[39m\n",
      "\u001b[33m|\u001b[39m                                           \u001b[33m|\u001b[39m\n",
      "\u001b[33m+-------------------------------------------+\n",
      "\u001b[39m\n",
      "Everything is up to date.                                                       \n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!dvc pull data/train.csv.dvc data/test.csv.dvc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34m.\u001b[00m\r\n",
      "├── README.md\r\n",
      "└── \u001b[01;34mdata\u001b[00m\r\n",
      "    ├── test.csv\r\n",
      "    ├── test.csv.dvc\r\n",
      "    ├── train.csv\r\n",
      "    └── train.csv.dvc\r\n",
      "\r\n",
      "1 directory, 5 files\r\n"
     ]
    }
   ],
   "source": [
    "!tree ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Information about remotes is stored in DVC config files (.dvc/config). Let’s get back to the original repository and look at how the configuration file changed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['remote \"localremote\"']\r\n",
      "url = ../titanic-remote\r\n",
      "[core]\r\n",
      "remote = localremote\r\n"
     ]
    }
   ],
   "source": [
    "!cat ../.dvc/config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now commit the changes to DVC config:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/mac/Desktop/sideProjects/learnML\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git add .dvc/config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[master bc5ad3e] add local remote\r\n",
      " 1 file changed, 4 insertions(+)\r\n"
     ]
    }
   ],
   "source": [
    "!git commit -m \"add local remote\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note**, that the newly created remote is local and thus its configuration may be kept outside of Git. DVC allows user to have several types of configuration, and the one called local is excluded from Git tracking. To use local configuration when creating remotes, just add `— local` option to `dvc remote add` command. We added this option for titanic-dvc-copy repository for illustration.\n",
    "\n",
    "> The same `— local` option should be used when creating **cloud-backed remotes**, as you’ll need to add credentials to access AWS S3, Azure or GCP remotes and it’s not recommended to have them in Git."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Versioning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, we used DVC to only add files to cache and remote storage. This is only a part of the story. More importantly, data files can be versioned. Of course, Titanic dataset would not change, but real datasets can change over time.\n",
    "\n",
    "We will simulate changes in data by just renaming Name column to FullName in both data files. This is enough for our purposes, as DVC doesn’t care, what actually changed, it just tracks the changes.\n",
    "\n",
    "For convenience, let’s tag the latest Git commit so that we can easily checkout files from it without messing with hashes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git tag base-dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now add edited data files to DVC:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m+-------------------------------------------+\n",
      "\u001b[39m\u001b[33m|\u001b[39m                                           \u001b[33m|\u001b[39m\n",
      "\u001b[33m|\u001b[39m     Update available \u001b[31m0.77.3\u001b[39m -> \u001b[32m0.83.0\u001b[39m     \u001b[33m|\u001b[39m\n",
      "\u001b[33m|\u001b[39m       Run \u001b[33mpip\u001b[39m install dvc \u001b[34m--upgrade\u001b[39m       \u001b[33m|\u001b[39m\n",
      "\u001b[33m|\u001b[39m                                           \u001b[33m|\u001b[39m\n",
      "\u001b[33m+-------------------------------------------+\n",
      "\u001b[39m\n",
      "Stage is cached, skipping.                                                      \n",
      "100% Add|██████████████████████████████████|1.00/1.00 [00:01<00:00,  1.65s/file]\n",
      "\n",
      "To track the changes with git, run:\n",
      "\n",
      "\tgit add data/test.csv.dvc\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!dvc add data/train.csv data/test.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git add data/test.csv.dvc data/train.csv.dvc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[master 69340fa] rename dataset columns\n",
      " 1 file changed, 7 insertions(+)\n",
      " create mode 100644 data/test.csv.dvc\n"
     ]
    }
   ],
   "source": [
    "!git commit -m \"rename dataset columns\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: l'étiquette 'renamed-dataset' existe déjà\r\n"
     ]
    }
   ],
   "source": [
    "!git tag renamed-dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m+-------------------------------------------+\n",
      "\u001b[39m\u001b[33m|\u001b[39m                                           \u001b[33m|\u001b[39m\n",
      "\u001b[33m|\u001b[39m     Update available \u001b[31m0.77.3\u001b[39m -> \u001b[32m0.83.0\u001b[39m     \u001b[33m|\u001b[39m\n",
      "\u001b[33m|\u001b[39m       Run \u001b[33mpip\u001b[39m install dvc \u001b[34m--upgrade\u001b[39m       \u001b[33m|\u001b[39m\n",
      "\u001b[33m|\u001b[39m                                           \u001b[33m|\u001b[39m\n",
      "\u001b[33m+-------------------------------------------+\n",
      "\u001b[39m\n",
      "\u001b[0m                                                                            "
     ]
    }
   ],
   "source": [
    "!dvc push data/train.csv.dvc data/test.csv.dvc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34m../.dvc/cache\u001b[00m\r\n",
      "├── \u001b[01;34m02\u001b[00m\r\n",
      "│   └── 9c9cd22461f6dbe8d9ab01def965c6\r\n",
      "├── \u001b[01;34m61\u001b[00m\r\n",
      "│   └── fdd54abdbf6a85b778e937122e1194\r\n",
      "├── \u001b[01;34m8b\u001b[00m\r\n",
      "│   └── 686a9e84994cb693999608b9201619\r\n",
      "└── \u001b[01;34mee\u001b[00m\r\n",
      "    └── f968ede3c2968e56e886c6b94d265a\r\n",
      "\r\n",
      "4 directories, 4 files\r\n"
     ]
    }
   ],
   "source": [
    "!tree ../.dvc/cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the moment, data files in the working directory contain renamed columns, and DVC added them to cache (check this with `tree .dvc/cache`) and we pushed it to the local remote. So far, all the changes were propagated to all locations.\n",
    "\n",
    "Lets’ assume, that you want to get the original version of the data. We intentionally tagged the corresponding commit, and now can easily checkout DVC metafiles for that version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 chemins mis à jour depuis 3792fb7\r\n"
     ]
    }
   ],
   "source": [
    "!git checkout base-dataset data/train.csv.dvc data/test.csv.dvc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the metafiles for `base-dataset`, we can easily get the original version of the data files from cache or remote storage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m+-------------------------------------------+\n",
      "\u001b[39m\u001b[33m|\u001b[39m                                           \u001b[33m|\u001b[39m\n",
      "\u001b[33m|\u001b[39m     Update available \u001b[31m0.77.3\u001b[39m -> \u001b[32m0.83.0\u001b[39m     \u001b[33m|\u001b[39m\n",
      "\u001b[33m|\u001b[39m       Run \u001b[33mpip\u001b[39m install dvc \u001b[34m--upgrade\u001b[39m       \u001b[33m|\u001b[39m\n",
      "\u001b[33m|\u001b[39m                                           \u001b[33m|\u001b[39m\n",
      "\u001b[33m+-------------------------------------------+\n",
      "\u001b[39m\n",
      "\u001b[33mWARNING\u001b[39m: data 'data/test.csv' exists. Removing before checkout.       \n",
      "\u001b[33mWARNING\u001b[39m: data 'data/train.csv' exists. Removing before checkout.      \n",
      "\u001b[0m                                                                            "
     ]
    }
   ],
   "source": [
    "!dvc checkout data/train.csv.dvc data/test.csv.dvc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you checkout the data files, you will see, that column is named Name, as it was in the original files. To revert data files to the current form with FullName instead of Name, just reset corresponding metafiles to `Git HEAD`and perform `dvc checkout` again.\n",
    "\n",
    "> Note, that with DVC we effectively version control metafiles. All the tracking of actual data files is performed by DVC based on information in metafiles.\n",
    "\n",
    "As you can see, DVC is convenient and simple enough to be used for data versioning. It allows to easily record the state of the data, and switch between different versions (with some help from Git).\n",
    "\n",
    "This reduces the mess and helps to keep data coherent across teammates and locations. However, DVC can do more: it can track calculations and results, allowing to recreate any previous result without a lot of trouble."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Managing calculations with DVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DVC has two main concepts for reproducible calculations: **stages** and **pipelines**. \n",
    "\n",
    "Let’s start with the simpler one and create a DVC stage, which calculates some features. We will not go too far right now, and will just make some columns categorical (see `pytitanic/features.py`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/mac/Desktop/sideProjects/learnML'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/mac/Desktop/sideProjects/learnML/titanic-dvc\n"
     ]
    }
   ],
   "source": [
    "cd titanic-dvc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "!touch pytitanic/__init__.py pytitanic/features.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_dummy = pd.read_csv(\"./data/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived  Pclass  \\\n",
       "0            1         0       3   \n",
       "1            2         1       1   \n",
       "2            3         1       3   \n",
       "3            4         1       1   \n",
       "4            5         0       3   \n",
       "\n",
       "                                                Name     Sex   Age  SibSp  \\\n",
       "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
       "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
       "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
       "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
       "4                           Allen, Mr. William Henry    male  35.0      0   \n",
       "\n",
       "   Parch            Ticket     Fare Cabin Embarked  \n",
       "0      0         A/5 21171   7.2500   NaN        S  \n",
       "1      0          PC 17599  71.2833   C85        C  \n",
       "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
       "3      0            113803  53.1000  C123        S  \n",
       "4      0            373450   8.0500   NaN        S  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dummy.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 891 entries, 0 to 890\n",
      "Data columns (total 12 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   PassengerId  891 non-null    int64  \n",
      " 1   Survived     891 non-null    int64  \n",
      " 2   Pclass       891 non-null    int64  \n",
      " 3   Name         891 non-null    object \n",
      " 4   Sex          891 non-null    object \n",
      " 5   Age          714 non-null    float64\n",
      " 6   SibSp        891 non-null    int64  \n",
      " 7   Parch        891 non-null    int64  \n",
      " 8   Ticket       891 non-null    object \n",
      " 9   Fare         891 non-null    float64\n",
      " 10  Cabin        204 non-null    object \n",
      " 11  Embarked     889 non-null    object \n",
      "dtypes: float64(2), int64(5), object(5)\n",
      "memory usage: 83.7+ KB\n"
     ]
    }
   ],
   "source": [
    "df_dummy.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       NaN\n",
       "1       C85\n",
       "2       NaN\n",
       "3      C123\n",
       "4       NaN\n",
       "       ... \n",
       "886     NaN\n",
       "887     B42\n",
       "888     NaN\n",
       "889    C148\n",
       "890     NaN\n",
       "Name: Cabin, Length: 891, dtype: object"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dummy[\"Cabin\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dummy[\"CabinId\"] = df_dummy[\"Cabin\"].str.get(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      NaN\n",
       "1        C\n",
       "2      NaN\n",
       "3        C\n",
       "4      NaN\n",
       "      ... \n",
       "886    NaN\n",
       "887      B\n",
       "888    NaN\n",
       "889      C\n",
       "890    NaN\n",
       "Name: CabinId, Length: 891, dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dummy[\"CabinId\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     -1\n",
       "1      2\n",
       "2     -1\n",
       "3      2\n",
       "4     -1\n",
       "      ..\n",
       "886   -1\n",
       "887    1\n",
       "888   -1\n",
       "889    2\n",
       "890   -1\n",
       "Length: 891, dtype: int8"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dummy[\"CabinId\"].astype(\"category\").cat.codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code is simple and self-explanatory so we would not go through it. In a typical environment, you would immediately launch `python -m pytitanic.features …`, but with DVC it works a bit differently.\n",
    "First, let’s add newly created Python files to Git:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git add pytitanic/features.py pytitanic/__init__.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[master 5c7f5a9] Basic features calculation added.\r\n",
      " 4 files changed, 42 insertions(+), 4 deletions(-)\r\n",
      " create mode 100644 pytitanic/__init__.py\r\n",
      " create mode 100644 pytitanic/features.py\r\n"
     ]
    }
   ],
   "source": [
    "!git commit -m \"Basic features calculation added.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let’s create a DVC stage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/mac/Desktop/sideProjects/learnML/titanic-dvc\n"
     ]
    }
   ],
   "source": [
    "cd titanic-dvc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running command:                                                        \n",
      "\tpython3 -m pytitanic.features -o features -r data/train.csv -s data/test.csv\n",
      "                                                                        \n",
      "To track the changes with git, run:\n",
      "\n",
      "\tgit add features/features.dvc features/.gitignore\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!dvc run -f features/features.dvc \\\n",
    "                     -d data/train.csv -d data/test.csv -d pytitanic/features.py \\\n",
    "                     -o features/train_features.csv \\\n",
    "                     -o features/test_features.csv \\\n",
    "                     python3 -m pytitanic.features \\\n",
    "                     -o features -r data/train.csv -s data/test.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s decrypt this. First, we instruct DVC to run command and create a stage (and a corresponding stage file) with `dvc run -f features/features.dvc`. Stage in **DVC is just a chunk of tracked computation**. All information about how stage is performed is stored in the stage file (`features/features.dvc` in this case).\n",
    "\n",
    "To tell DVC about dependencies, we use `-d` option and list all the files needed for the computation. With `-o` we provide outputs of the stage. The final part is the command itself.\n",
    "\n",
    "As we launch the command above, DVC will create the stage file, which contains all the information needed to recreate the stage results at any time in the future:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, that DVC adds output files to the cache, so you do not need to do this manually. However, we need to commit stage file and `.gitignore` created by DVC in features directory (again, .gitignore was added by DVC to exclude output files from Git tracking) and create a tag for convenience:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git add features/features.dvc features/.gitignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[master 9ef7023] Basic features calculated.\r\n",
      " 2 files changed, 23 insertions(+)\r\n",
      " create mode 100644 features/.gitignore\r\n",
      " create mode 100644 features/features.dvc\r\n"
     ]
    }
   ],
   "source": [
    "!git commit -m \"Basic features calculated.\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git tag base-features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may ask, what’s the point here? Can’t we just put output files under DVC control manually as we did before? Yes, we can, but now we not only have them in cache (and so can push to any remote to share with others) but also can easily recreate the calculation with updated code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculations, however, may be more complex than just a single stage. In that case, we want some modularity instead of a single monolitic computation.\n",
    "\n",
    "DVC has tools for that too. To illustrate that, we will now proceed to actual machine learning tools. For that, we will create a simple CatBoost model with the features we have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "!touch pytitanic/model.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file looks large, but it’s very straightforward. First, we create a stratified random split. We can stratify on any categorical column, but let’s use Pclass as the default (think for a moment - why are we doing this?). We then perform some missing values imputation using the training set and finally, we train the model. All the results are stored on disk.\n",
    "\n",
    "Several things to note:\n",
    "- **parameters are explicit** and captured in the command line. This allows us to have reproducible commands without any implicit defaults,\n",
    "- this script creates what is called **metrics file** alongside model file and submission. We will see later, how useful this is in a combination with DVC,\n",
    "- **random state is explicit**, and can be provided as a command-line parameter which has a default value. We won’t lose track of it.\n",
    "    \n",
    "We are now ready to train the model using dvc run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git add pytitanic/model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[master 105fd1c] ML model training added.\r\n",
      " 1 file changed, 85 insertions(+)\r\n",
      " create mode 100644 pytitanic/model.py\r\n"
     ]
    }
   ],
   "source": [
    "!git commit -m \"ML model training added.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.84.0\r\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!dvc --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!dvc run -f results/model.dvc \\ \n",
    "                     -d features/train_features.csv \\ \n",
    "                     -d features/test_features.csv \\ \n",
    "                     -d pytitanic/model.py \\ \n",
    "                     -o results/cb-model.cbm \\ \n",
    "                     -o results/cb-submission.csv \\ \n",
    "                     -m results/cb-metrics.json \\ \n",
    "                     python3 -m pytitanic.model -o results \\ \n",
    "                     -r features/train_features.csv -s features/test_features.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we actually created is a DVC *pipeline*. To generate the output required, DVC checks if all the dependencies are at place since features/train_features.csv and features/test_features.csv are themselves results of another stage (features/festures.dvc). Now that we have defined the stage for results/model.dvc, we can use it with dvc repro. Additionally, you can inspect a pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results/model.dvc\n",
      "\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!dvc pipeline show --tree results/model.dvc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When DVC tries to reproduce results/model.dvc, it first constructs a dependencies graph and decides on whether any of the dependencies must be reproduced. Right now they are all up to date and DVC does not perform any calculations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING\u001b[39m: DVC-file 'results/model.dvc' is a \"callback\" stage (has a command and no dependencies) and thus always considered as changed.\n",
      "\u001b[33mWARNING\u001b[39m: Stage 'results/model.dvc' changed.\n",
      "Running command:\n",
      "\t \n",
      "                                                                        \n",
      "To track the changes with git, run:\n",
      "\n",
      "\tgit add results/model.dvc\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!dvc repro results/model.dvc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, if we remove one or both of the features files, DVC will recognize that and reproduce them first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm features/*_features.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING\u001b[39m: DVC-file 'results/model.dvc' is a \"callback\" stage (has a command and no dependencies) and thus always considered as changed.\n",
      "\u001b[33mWARNING\u001b[39m: Stage 'results/model.dvc' changed.\n",
      "Running command:\n",
      "\t \n",
      "                                                                        \n",
      "To track the changes with git, run:\n",
      "\n",
      "\tgit add results/model.dvc\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!dvc repro results/model.dvc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">You may note somewhat unfortunate name of dvc repro command. It does not reproduce the target in the scientific sense but rather recalculates it. To reproduce one of the previous versions you either should checkout it from DVC cache, or checkout corresponding Git commit and actually recalculate it again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, DVC goes back through all the stages we defined so far and recognizes that some of the intermediate results needed for results/model.dvc are missing. It reproduces features/features.dvc stage, but correctly determines, that the actual reproduced files are up to date, so there’s no need to either save them to cache or to reproduce results/model.dvc.\n",
    "\n",
    "Another ingredient is the metrics file. DVC can track metrics alongside other outputs. This allows to later recall the performance of the model:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><p style='color:red'> ALERT: </p></b>\n",
    "I don't know if the problem is in my environment but if I don't run the last python command alone, then no training/ file output is done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!dvc pipeline show --tree results/model.dvc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!dvc repro results/model.dvc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!dvc metrics show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: Best run all the commands in a cmd and not in jupyter notebook.\n",
    "\n",
    "If you run into a dvc lock problem run: `dvc init -f -v`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moreover, DVC can fetch specific metrics directly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! dvc metrics show -x AUC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">In this case, DVC recognizes, that the metrics file is in JSON format, and uses path (-x or — xpath) to find the actual field inside the file.\n",
    "\n",
    "We can now commit the newly created stage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git add results/model.dvc results/.gitignore\n",
    "!git commit -m \"Basic CatBoost model created.\"\n",
    "!dvc push"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running experiments\n",
    "\n",
    "With stages, pipelines and metrics files, DVC allows performing even more flexible operations. Consider this: after you’ve created the basic features and your first model, you want to add some additional features and retrain the model. You want to determine if the new features are better. How would you achieve this?\n",
    "\n",
    "The workflow may be as follows:\n",
    "- **create a new Git branch**,\n",
    "- **add new code** to calculate additional features,\n",
    "- **reproduce** the pipeline for `results/model.dvc`\n",
    "- **compare metrics** for the model on initial features with the one, trained on new features.\n",
    "\n",
    "Let’s try this out. First, we create a branch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git checkout -b experimental-features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we add a new feature (well known PclassSex) in `pytitanic/features.py`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now need to reproduce `results/model.dvc`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DVC will report, that since one of the dependencies changed, the stage must be recreated and then will launch the calculation. Both `features/features.dv`c and `results/model.dvc` will be updated and we can commit it now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic-dvc> git add pytitanic/features.py features/features.dvc results/model.dvc\n",
    "titanic-dvc> git commit -m \"Combination of class and sex added as a feature.\"\n",
    "titanic-dvc> git tag pclass-sex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now launch `dvc metrics` and see, how the newly created model compares with the previous one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!dvc metrics show -a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DVC has an option for `dvc metrics` show to show all available metrics over all branches, namely -a. This helps us to compare metrics either between different models inside a branch or different versions of the same model. Here we can see, that the new model is slightly better in terms of accuracy.\n",
    "\n",
    "We may now decide to merge this branch back to master or submit both submission files to Kaggle to compare leaderboard metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Moving forward\n",
    "\n",
    "DVC is a powerful tool and we covered only the fundamentals of it. DVC can be more flexible: it can be configured to use links between the working directory and cache to save space, can use any of three main cloud providers for remote storage, or even install Git hooks.\n",
    "\n",
    "However, similar to Git, DVC is easy to introduce into daily practice with a set of simple rules:\n",
    "- once you started a project, add each original data file to DVC with dvc add,\n",
    "- create artefacts (intermediate data files, model, etc.) only using DVC stages or pipelines,\n",
    "- commit corresponding *.dvc metafiles to Git,\n",
    "- record metrics when running calculations or training with dvc run,\n",
    "- compare different experiments with dvc metrics show,\n",
    "- remember, that DVC does not assign any special meaning to metrics, and you can store any important information as metrics; for example, you may want to store information about running time performance of a calculation,\n",
    "- to move through history and reproduce earlier results, use a combination of git checkout, dvc checkout (with optional dvc pull) and dvc repro,\n",
    "- share your data files through convenient remote storage with dvc push."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datascience_devenv",
   "language": "python",
   "name": "datascience_devenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
